from torch import nn
class MultiHeadAttention(nn.Module):
    def __init__(self, dim, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        pass
    def forward(self, q, k, v):
        pass